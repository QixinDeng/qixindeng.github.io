<!doctype html>
<html>
  <head>
  <script src="https://use.fontawesome.com/baff6f55f5.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Qixin Deng </title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-29643011-3', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- For all browsers -->
    <link rel="stylesheet" href="assets/css/academicons.min.css"/>
    <link rel="stylesheet" href="assets/css/academicons.css"/>
    
    <style>
      button.accordion {
      font:14px/1.5 Lato, "Helvetica Neue", Helvetica, Arial, sans-serif;
      cursor: pointer;
      padding: 0px;
      border: none;
      text-align: left;
      outline: none;
      font-size: 100%;
      transition: 0.3s;
      background-color: #f8f8f8;
      }
      button.accordion.active, button.accordion:hover {
      background-color: #f8f8f8;
      }
      button.accordion:after {
      content: " [+] ";
      font-size: 90%;
      color:#777;
      float: left;
      margin-left: 1px;
      }

      button.accordion.active:after {
      content: " [\2212] ";
      }
      div.panel {
      padding: 0 20px;
      margin-top: 5px;
      display: none;
      background-color: white;
      font-size: 100%;
      }
      div.panel.show {
      display: block !important;
      }
    </style>
  </head>
  <body>
    <div class="wrapper">
      <header>
        <p><small><a href="https://qixindeng.github.io/index.html"></a></p>
        <h1><a href="https://qixindeng.github.io/google8e6b810b19bdc5eb.html">Qixin Deng's Website</a></h1>
        <img src="research/qixin.png" height="400" width="320">
        <p><br>Ph.D. Candidate at <br>Dept. of Computer Science, <br>University of Houston, <br>Houston, TX, USA</br></p>
    <h3><p class="view"><a href="https://qixindeng.github.io/research/QIxin_ Deng_Resume.pdf">Resume</a></p></h3>  
    <p class="view"><b>Social</b><br>
        <a href="mailto:qxdeng1991[at]gmail[dot]com" class="author-social" target="_blank"><i class="fa fa-fw fa-envelope-square"></i> Email</a> qxdeng1991@gmail.com<br>
        <a href="https://scholar.google.com/citations?user=DtjBcf8AAAAJ&hl=en" target="_blank"><i class="ai ai-fw ai-google-scholar-square"></i> Google Scholar</a><br>
        <a href="https://www.linkedin.com/in/qixin-deng-5b1959157/" class="author-social" target="_blank"><i class="fa fa-fw fa-linkedin-square"></i> LinkedIn</a><br>
      </header>
      <section>

    <h3>About Me</h3>
      <p>
      I am a Ph.D. candidate in Computer Science Department from <a href="http://uh.edu/">University of Houston</a>, TX, USA, under supervision of <a href="http://graphics.cs.uh.edu/zdeng/">Dr. Zhigang Deng</a>. I'm to graduate in May 2023 and actively seeking software engineer/applied scientist positions.  My research primarily centers on human face reconstruction, facial modeling and multi-party animation. I've been working in <a href="http://graphics.cs.uh.edu/people/">Computer Graphics and Interactive Media Lab</a> since 01/2018.
      </p>
             
        <hr>

<!--  -----------------------------------------------------------------     -->

  <h2><a id="recent-RRs-updated" class="anchor" href="#RRpapers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Education</h2>

<div class="contentpanel">
      <!-- The title of the panel -->
      <p id="pcontent">
        Jan. 2018 -- May. 2023 <b>Ph.D.</b> in Computer Science, <a href="http://uh.edu/">University of Houston</a>, TX, USA </br>
        Aug. 2015 -- May. 2017 <b>M.S.</b> in Electrical and Computer Engineering, <a href="https://www.purdue.edu/">Purdue University</a>, IN, USA </br>
        Aug. 2010 -- Jul. 2014 <b>B.S.</b> in Electronic Information Engineering, <a href="http://english.zzu.edu.cn/">Zhengzhou University</a>, China </br> 
      </p>
    </div>


   <h2><a id="recent-RRs-updated" class="anchor" href="#RRpapers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Publications</h2>

 <!--  -----------------------------------------------------------------     -->
  <p>
        <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold">
        S2M-Net: Speech Driven Three-party Conversational Motion Synthesis Networks </a> 
        <br> Aobo Jin, <b> Qixin Deng</b>, and Zhigang Deng. 
        <br>Proceeding of ACM SIGGRAPH Conference on Motion, Interaction, and Games (MIG) 2022, Guanajuato, Mexico, Nov 2022.
      
        <br>[ <a style="margin:0; font-size:100%; font-weight:bold" href="http://graphics.cs.uh.edu/wp-content/papers/2022/2022-MIG-S2M-Net.pdf">paper</a>  ]
     	    [ <a style="margin:0; font-size:100%; font-weight:bold" href="https://www.youtube.com/watch?v=RJaDCgyqd_g">video</a>  ]
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
    In this paper we propose a novel conditional generative adversarial network (cGAN) architecture, called S2M-Net, to holistically synthesize realistic three-party conversational animations based on acoustic speech input together with speaker marking (i.e., the speaking time of each interlocutor). Specifically, based on a pre-collected three-party conversational motion dataset, we design and train the S2M-Net for three-party conversational animation synthesis. In the architecture, a generator contains a LSTM encoder to encode a sequence of acoustic speech features to a latent vector that is further fed into a transform unit to transform the latent vector into a gesture kinematics space. Then, the output of this transform unit is fed into a LSTM decoder to generate corresponding three-party conversational gesture kinematics. Meanwhile, a discriminator is implemented to check whether an input sequence of three-party conversational gesture kinematics is real or fake. To evaluate our method, besides quantitative and qualitative evaluations, we also conducted paired comparison user studies to compare it with the state of the art. </div></p>
  </p>
 <!--  -----------------------------------------------------------------     -->



 <!--  -----------------------------------------------------------------     -->
  <p>
        <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold">
        End-to-End 3D Face Reconstruction with Expressions and Specular Albedos From Single In-the-wild Images </a> 
        <br><b> Qixin Deng</b>, Binh Huy Le, Aobo Jin, and Zhigang Deng. 
        <br>Proceeding of ACM International Conference on Multimedia (MM) 2022, Lisbon, Portugal, Oct 2022
      
        <br>[ <a style="margin:0; font-size:100%; font-weight:bold" href="http://graphics.cs.uh.edu/wp-content/papers/2022/2022-MM-EndtoEnd3DFaceReconstruction.pdf">paper</a>  ]
     	    [ <a style="margin:0; font-size:100%; font-weight:bold" href="https://www.youtube.com/watch?v=tsbNI9vj1Ec&t=17s">video</a>  ]
	    [ <a class="view";style="margin:0; font-size:100%; font-weight:bold" href="https://qixindeng.github.io/research/results.pdf">supplementary material</a>]
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
    Recovering 3D face models from in-the-wild face images has numerous potential applications.
However, properly modeling complex lighting effects in reality, including specular lighting, shadows, and occlusions, from a single in-the-wild face image is still considered as a widely open research challenge. In this paper, we propose a convolutional neural network based framework to regress the face model from a single image in the wild. The outputted face model includes dense 3D shape, head pose, expression, diffuse albedo, specular albedo, and the corresponding lighting conditions. Our approach uses novel hybrid loss functions to disentangle face shape identities, expressions, poses, albedos, and lighting. Besides a carefully-designed ablation study, we also conduct direct comparison experiments to show that our method can outperform state-of-art methods both quantitatively and qualitatively. </div></p>
 </p>
 <!--  -----------------------------------------------------------------     -->
        


        <!--  -----------------------------------------------------------------     -->
        <p>
        <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://direct.mit.edu/pvar/article-abstract/doi/10.1162/pres_a_00358/111797/A-Live-Speech-Driven-Avatar-Mediated-Three-Party?redirectedFrom=fulltext">
          A Live Speech Driven Avatar-mediated Three-party Telepresence System: Design and Evaluation</a> 
          <br>Aobo Jin*, <b> Qixin Deng* </b>, Luming Ma, and Zhigang Deng. 
          <br>PRESENCE: Virtual and Augmented Reality, MIT press, accepted in June 2022
      
      <br>[ <a style="margin:0; font-size:100%; font-weight:bold" href="http://graphics.cs.uh.edu/wp-content/papers/2022/2022-Telepresence-LiveSpeechDrivenSystem.pdf">paper</a>  ]
     
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
    In this paper, we present a live {speech-driven}, avatar-mediated, three-party telepresence system, through which three distant users, embodied as avatars in a shared 3D virtual world, can perform natural three-party telepresence that does not require tracking devices. Based on live speech input from three users, this system can real-time generate {the} corresponding conversational motions of all the avatars, including head motion, eye motion, lip movement, torso motion, and hand gesture. All motions are generated automatically at each user side based on live speech input, and a cloud server is utilized to transmit and synchronize motion and speech among different users. We conduct a formal user study to evaluate the usability and effectiveness of the system by comparing it with a well-known online virtual world, {\it Second Life}, and a widely-used online teleconferencing system, {\it Skype}. The user study results indicate our system can provide a measurably better telepresence user experience than the two widely-used methods. </div></p>
  </p>       
  <!--  -----------------------------------------------------------------     -->
        
        
        <!--  -----------------------------------------------------------------     -->
        <p>
        <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://ieeexplore.ieee.org/document/9321747">
          Plausible 3D Face Wrinkle Generation Using Variational Autoencoders</a> 
          <br><b> Qixin Deng </b>, Luming Ma, Aobo Jin, Huikun Bi, Binh Huy Le, and Zhigang Deng. 
          <br>IEEE Transactions on Visualization and Computer Graphics(TVCG) 28.9 (2021): 3113-3125.
      
      <br>[ <a style="margin:0; font-size:100%; font-weight:bold" href="http://graphics.cs.uh.edu/wp-content/papers/2021/2021-TVCG-FaceWrinkleGeneration.pdf">paper</a> 
      ]
        [ <a style="margin:0; font-size:100%; font-weight:bold" href="https://www.youtube.com/watch?v=yUeyuyKmkpQ">video</a> 
      ]

        
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
Realistic 3D facial modeling and animation have been increasingly used in many graphics, animation, and virtual reality applications. However, generating realistic fine-scale wrinkles on 3D faces, in particular, on animated 3D faces, is still a challenging problem that is far away from being resolved. In this paper we propose an end-to-end system to automatically augment coarse-scale 3D faces with synthesized fine-scale geometric wrinkles. By formulating the wrinkle generation problem as a supervised generation task, we implicitly model the continuous space of face wrinkles via a compact generative model, such that plausible face wrinkles can be generated through effective sampling and interpolation in the space. We also introduce a complete pipeline to transfer the synthesized wrinkles between faces with different shapes and topologies. Through many experiments, we demonstrate our method can robustly synthesize plausible fine-scale wrinkles on a variety of coarse-scale 3D faces with varied shapes and expressions.</div></p>
<!--     <img src="research/EG_survey.png.jpg" height="180" width="480">  -->
  </p>       
  <!--  -----------------------------------------------------------------     -->
        
        
        <!--  -----------------------------------------------------------------     -->
        <p>
        <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://dl.acm.org/doi/10.1145/3340250">
          A Deep Learning based Model for Head and Eye Motion Generation in Three-party Conversations</a> 
          <br>Aobo Jin,<b> Qixin Deng </b>, Yuting Zhang, and Zhigang Deng. 
          <br>Proceedings of the ACM on Computer Graphics and Interactive Techniques(SCA) 2.2 (2019): 1-19.
      
      <br>[ <a style="margin:0; font-size:100%; font-weight:bold" href="http://graphics.cs.uh.edu/wp-content/papers/2019/2019-SCA-ThreePartyEyeHead.pdf">paper</a> 
      ]
      [ <a style="margin:0; font-size:100%; font-weight:bold" href="https://www.youtube.com/watch?v=spzKJhOHChI">video</a> 
      ]
        
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
In this paper we propose a novel deep learning based approach to generate realistic three-party head and eye motions based on novel acoustic speech input together with speaker marking (i.e., speaking time for each interlocutor). Specifically, we first acquire a high quality, three-party conversational motion dataset. Then, based on the acquired dataset, we train a deep learning based framework to automatically predict the dynamic directions of both the eyes and heads of all the interlocutors based on speech signal input. Via the combination of existing lip-sync and speech-driven hand/body gesture generation algorithms, we can generate realistic three-party conversational animations. Through many experiments and comparative user studies, we demonstrate that our approach can generate realistic three-party head-and-eye motions based on novel speech recorded on new subjects with different genders and ethnicities.</div></p>
<!--     <img src="research/EG_survey.png.jpg" height="180" width="480">  -->

<h2><a id="recent-RRs-updated" class="anchor" href="#RRpapers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Work Experience</h2>

<div class="contentpanel">
      <!-- The title of the panel -->
      <p id="pcontent">
        May 2022 -- Aug. 2022 Research & Development Intern at Electronic Arts SEED LAB, <a href="https://www.ea.com/seed">Electronic Arts</a> Austin, TX </br> 
      </p>
    </div>
    
<h2><a id="recent-RRs-updated" class="anchor" href="#RRpapers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Teaching Experience</h2>

<div class="contentpanel">
      <!-- The title of the panel -->
      <p id="pcontent">
        COSC4370-<b>Interactive Computer Graphics</b> (lectures on OpenGL Programming), Fall 2022, Fall 2021, Fall 2020</br> 
	COSC6372-<b>Computer Graphics</b> (lectures on OpenGL Programming, Exam Review), Spring 2021, Spring 2022 </br> 
      </p>
    </div>    

    
<h2><a id="recent-RRs-updated" class="anchor" href="#RRpapers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Award</h2>

<div class="contentpanel">
      <!-- The title of the panel -->
      <p id="pcontent">
        <b>Best PhD Award</b> (Department of Computer Science at University of Houston) 2022 </br>
        <b>President Scholarship</b> (Department of Computer Science at University of Houston) 2018 </br>
        <b>Outstanding Undergraduate Student</b> (Education Department of Henan Province,China) 2014 </br>
        <b>National Scholarship</b> (Ministry of Education of the People's Republic of China) 2012 </br>
      </p>
    </div>

</section>
      
    </div>
    <script src="javascripts/scale.fix.js"></script>
    <script> 
    var acc = document.getElementsByClassName("accordion");
    var i;

    for (i = 0; i < acc.length; i++) {
        acc[i].onclick = function(){
            this.classList.toggle("active");
            this.parentNode.nextElementSibling.classList.toggle("show");
      }
    }
    </script>
  </body>
</html>
